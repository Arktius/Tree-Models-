---
title: 'ML2: Exam Preparation'
author: "Denis Baskan"
date: "10 March 2019"
output:
  pdf_document: default
  html_notebook: default
  html_document: default
---


# Exercise: Tree Models
In the following code, a Regression tree and Classification tree will be applied. Some parts are from the book James et. al. Lab 8.3.2.

## Load Packages
If packages are not installed, then they will be installed.


```{r warning = FALSE}
required.packages =  c('MASS','rpart','rpart.plot','ROCR')

load.packages <- function(packages){
  
  for (pckg in packages){
    if (!(pckg %in% installed.packages()[,"Package"])){
      install.packages(pckg)
    }
    
    library(pckg, character.only = TRUE)
  }
}

load.packages(required.packages)
attach(Boston)
```

# Show Boston data set and some scatterplots
The data set contains data on housing values and other information about Boston suburbs.

```{r warning = FALSE}

#?Boston # shows a description of the data set and the columns
head(Boston)
cat("Number of rows: ", nrow(Boston))
cat("Number of columns: ", ncol(Boston)) 
cat("Column names: ", colnames(Boston)) 

plot(black,crim,main="Relationship between blacks and crime rate (by town)", xlab="Blacks", ylab="Crime rate", pch=19)
plot(tax,crim,main="Relationship between paid taxes and crime rate (by town and /$10,000)", xlab="Taxes", ylab="Crime rate", pch=19)
plot(rm,medv,main="Relationship between rooms and owner-occupied homes (in /$1000s)", xlab="Average Rooms", ylab="Owner-Occupied Homes", pch=19)

```



## Any suburbs with particularly high crime rate? Tax rates? Pupil-teacher ratios?
```{r warning = FALSE}

#x-axis has no meaning

plot(Boston[order(Boston$crim),]$crim,main="Crime Rate", xlab="Suburb", ylab="Crime rate (by town)", pch=19,type='l')
plot(Boston[order(Boston$tax),]$tax,main="Taxes", xlab="Suburb", ylab="Taxes", pch=19,type='l')
plot(Boston[order(Boston$ptratio),]$ptratio,main="Pupil-Teacher ratio", xlab="Suburb", ylab="Pupil-Teacher ratio", pch=19,type='l')
```

One can observe suburbs with really high values and almost exponential slope for the column crime rate. 

## Some Statistics
```{r warning = FALSE}

cat("Number of suburbs bound the Charles river: ",sum(Boston$chas==1))
cat("Median value of pupil-teacher ratio : ",median(Boston$ptratio))
cat("Lowest median value of owner-occupied homes : ",min(Boston$medv))
cat("Number of suburbs with more than 7 rooms per dwelling: ",sum(Boston$rm > 7))
cat("Number of suburbs with more than 8 rooms per dwelling: ",sum(Boston$rm > 8))
Boston[Boston$rm > 8,] 


#compare some data
Boston[Boston$medv==min(Boston$medv),]
```
The data with more than 8 rooms have a low crime rate and high values for age, tax, black for example.
The 2 suburbs with the lowest medv values have high values for the predictors crime, black, tax, pt-ratio. One can conclude that populations with a lower status and cheap houses are at increased risk of being a crime victim. 



# Fit a Regression tree using column medv as outcome variable
```{r warning = FALSE}
#Note: outcome variable should be continuous. Exercise follows Lab 8.3.2 in James et al.
set.seed(1) #set a fix random generator to reproduce the same results next time
train = sample(1:nrow(Boston), nrow(Boston)/2)   #split data randomly
tree.boston = rpart(medv ~.,Boston,subset=train) # create a tree
print(tree.boston) #only 3 variables were used
rpart.plot(tree.boston)
```

The tree indicates that a higher socioeconomic status leads in buying more expensive houses. A median house price of $46,000 can be observed when lstat is lower than 9.7% and number of rooms are higher than 7.4 on average.  

## Can pruning the tree improve our model?
Rule: Choose the smalles number of nodes (largest cp value) which lies within 1 std. dev. of the smallest deviance, i.e. lies below the dotted line.
```{r warning = FALSE}
printcp(tree.boston)
plotcp(tree.boston)
```

cp=0.016 lies below the dotted line and could improve our model

## Prune the tree and compare models
```{r warning = FALSE}
prune.boston = prune(tree.boston,cp=0.016)
prune.boston
rpart.plot(prune.boston)

#compare models by calculating MSE (Mean Squarred Error)
pred.train<-predict(tree.boston,newdata=Boston[train,])
mean((Boston$medv[train]-pred.train)^2)
pred.train.prune<-predict(prune.boston,newdata=Boston[train,])
mean((Boston$medv[train]-pred.train.prune)^2)

```


## Calculate the MSE for the test set 

```{r warning = FALSE}
pred.test<-predict(tree.boston,newdata=Boston[-train,])
mean((Boston$medv[-train]-pred.test)^2)
pred.test<-predict(prune.boston,newdata=Boston[-train,])
mean((Boston$medv[-train]-pred.test)^2)
```
Pruned tree performes slightly worse applied on train and test set, but we gained a simpler model. Taking the square root of the test set MSE gives $5,000 rounded. That's the range where test prediction lay in.

## Plot observed median values medv agains predictions
```{r warning = FALSE}
boston.test=Boston[-train,"medv"]
plot(pred.test,boston.test)
abline(c(0,1))

```
# Classification Tree: Prostate Cancer

Did the cancer recur after surgical removal of the postate?

Used data set: stagec #from package rpart
```{r warning = FALSE}
head(stagec)
cat("Number of rows: ", nrow(stagec))
cat("Number of columns: ", ncol(stagec)) 
cat("Column names: ", colnames(stagec)) 
cat("Recurrence of the disease: ",table(stagec$pgstat))

```
The column pgstat is the outcome of interest.

## Factorize the outcome variable & show some plots

Instead of having numerical values, we would rather have "No" and "Prog". This makes reading the output easier and rpart will recognise that the outcome variable is a factor variable and so will use the Gini coefficient to calculate the loss statistic, in order to determine each splits. As pgstat is numeric rpart would assume that a regression tree is wanted, and will use mean square error for the loss function.


```{r warning = FALSE}
stagec$progstat <- factor(stagec$pgstat, levels = 0:1, labels = c("No", "Prog"))
plot(stagec$g2~progstat,data=stagec )
plot(stagec$age~progstat,data=stagec )
barplot(table(stagec$eet,stagec$progstat),beside=TRUE,legend.text=TRUE,main="eet" )
barplot(table(stagec$grade,stagec$progstat),beside=TRUE,legend.text=TRUE,main="grade" )
barplot(table(stagec$gleason,stagec$progstat),beside=TRUE,legend.text=TRUE,main="gleason" )
barplot(table(stagec$ploidy,stagec$progstat),beside=TRUE,legend.text=TRUE,main="ploidy" )


```

## Training set

We use the whole data set, because we only have 146 observations.

```{r warning = FALSE}
c.tree <- rpart(progstat ~ age + eet + g2 + grade + gleason + ploidy,data = stagec)
rpart.plot(c.tree)
print(c.tree)
```

## Pruning the Tree

As with the regression tree we should look to see if pruning the tree gives us a better model.

```{r warning = FALSE}
printcp(c.tree)
plotcp(c.tree)
c.pruned<-prune(c.tree,cp=0.076)
print(c.pruned)
rpart.plot(c.pruned)
```

## Make predictions

We now set alpha = 0.5 to get the most likely of the two outcomes
```{r warning = FALSE}
alpha <- 0.5 #0.4
stagec$predict<-(predict(c.pruned)[,2]>alpha)
tt<-table(stagec$progstat,stagec$predict)
print(tt)
sens<-tt[2,2]/sum(tt[2,])
spec<-tt[1,1]/sum(tt[1,])
cat("sensitivity: ",sens,"specificity: ", spec, "sum: ", sens+spec)
fpr <-tt[1,2]/sum(tt[,2]) #false-positive rate
```

The value of the specificity is fairly high with ~97%. It tells us how many healthy people were correctly identified as not having the condition. Whereas one cannot take into account the sensitivity. One might be more interested in detecting the patients with progression to consider further treatments. 

## Draw ROC diagram and AUC


```{r warning = FALSE}
p <- predict(c.pruned)[,2]
#rpart function to get the prediction for Yes
pr <- prediction(p, stagec$progstat) #convert the predictions into ROCR format
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
#ROCR function calculates everything for the ROC curve
plot(prf) #plot the ROC curve
abline(c(0,1))
points(fpr,sens)
AUC<-performance(pr, measure ="auc")@y.values[[1]];AUC
#RORC function calculates the AUC
```

Our model is above the diagonal and therefore better than tossing a coin. However, one might be rather interested in detecting the patients with progression. One can change the alpha value to get another model.
If you set alpha to 0.4, then your sensitivity becomes larger, but your specificity smaller at the same time. It is similar of saying 'progression is true' each time unless there are convinving values against it. If you toss a coin and say that 80% of the events are head, then you are really good at predicting head, but not for tail.














